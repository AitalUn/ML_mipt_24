**Week2: Sequence-to-sequence Learning, Neural Machine Translation**

Neural Machine Translation as seq2seq:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-course/blob/24s_advanced/week02_seq2seq_nmt/week02_seq2seq_for_nmt.ipynb)

Solved version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-course/blob/24s_advanced/week02_seq2seq_nmt/week02_seq2seq_for_nmt.solved.ipynb)


[Lecture slides](https://github.com/girafe-ai/ml-course/blob/24s_advanced/week02_seq2seq_nmt/week02_seq2seq_nmt.pdf)


Further readings:

- YSDA nlp_course materials:
  https://github.com/yandexdataschool/nlp_course/tree/2023/week04_seq2seq
- Great blog post by Jay Alammar:
  https://jalammar.github.io/illustrated-transformer/
- Notebook on positional encoding:
  [link](https://github.com/ml-mipt/ml-mipt/blob/advanced/week04_Transformer/week04_positional_encoding_carriers.ipynb)
- Great Annotated Transformer article with code and comments by Harvard NLP
  group: https://nlp.seas.harvard.edu/2018/04/03/attention.html
