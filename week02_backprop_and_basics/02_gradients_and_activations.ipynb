{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в глубокое обучение\n",
    "### Занятие 2. Метод обратного распространения ошибки, функции активации\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### План занятия:\n",
    "0. Повторение базовых понятий из мира нейронных сетей\n",
    "1. Введение в PyTorch\n",
    "2. Метод обратного распространения ошибки (backpropagation)\n",
    "3. Обзор различных функций активации\n",
    "4. Построение нейронной сети и сравнение с классическими методами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовые понятия при работе с нейронными сетями\n",
    "\n",
    "__Нейронная сеть (neural network)__ – композиция линейных и нелинейных преобразований. В целом, нейронная сеть представляет собой сложную (параметрическую) функцию $f$, задающую отображение из исходного признакового пространства $\\mathbb{X}_O$ в целевое пространство $\\mathbb{Y}$:\n",
    "$$\n",
    "f: \\;\\;\\; \\mathbb{X}_O \\longrightarrow \\mathbb{Y}.\n",
    "$$\n",
    "Часто нейронные сети представляют собой последовательность преобразований, представленных слоями и функциями активации.\n",
    "\n",
    "__Слой (layer)__ – некоторая функция/преобразование над исходными данными. Простейший пример: линейный слой, являющийся линейным преобразованием над входящими данными (т.е. просто преобразование $WX +b$, как и в линейной регрессии).\n",
    "\n",
    "__Функция активации (activation function)__ – нелинейное преобразование, применяющееся ко всем данным пришедшим на вход поэлементно. Благодаря функциям активации нейронные сети способны преобразовывать данные *нелинейным образом*, что позволяет порождать более информативные признаковые описания.\n",
    "\n",
    "__Функция потерь (loss function)__ – функция потерь, оценивающая качество полученного предсказания. Как правило, от функции потерь требуется свойство дифференцируемости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "### Введение в PyTorch\n",
    "\n",
    "[__PyTorch__](http://pytorch.org/) – один из основных фреймворков в DL на текущий момент. Его серьезным преимуществом является способность динамически строить граф вычислений. Все вычисления могут производиться в тот момент, когда вы к ним обращаетесь, что значительно упрощает как обучение, так и отладку моделей. Также он способен *автоматически вычислять градиенты*, *использовать GPU для ускорения вычислений* и в целом значительно упрощает разработку методов машинного обучения, основанных на нейронных сетях.\n",
    "\n",
    "Синтаксис в PyTorch во многом похож на Numpy. Рассмотрим подробнее различия и сходства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print('PyTorch version: {}'.format(torch.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим матрицу и применим к ней несколько базовых операций сначала с помощью numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4,4)\n",
    "\n",
    "print(\"X :\\n%s\\n\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print(\"add 5 :\\n%s\\n\" % (x + 5))\n",
    "print(\"X*X^T  :\\n%s\\n\" % np.dot(x,x.T))\n",
    "print(\"mean over cols :\\n%s\\n\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\\n\" % (np.cumsum(x,axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А затем с помощью PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4,4)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32) #or torch.arange(0,16).view(4,4)\n",
    "\n",
    "print (\"X :\\n%s\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print (\"add 5 :\\n%s\" % (x + 5))\n",
    "print (\"X*X^T  :\\n%s\" % torch.matmul(x,x.transpose(1,0)))  #short: x.mm(x.t())\n",
    "print (\"mean over cols :\\n%s\" % torch.mean(x,dim=-1))\n",
    "print (\"cumsum of cols :\\n%s\" % torch.cumsum(x,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, их синтаксис крайне похож, что значительно упрощает изучение и использование PyTorch.\n",
    "Стоит заметить, что совместимость [не является полной](https://github.com/pytorch/pytorch/issues/50344), и смешивать `numpy` и `torch` неявным образом не стоит.\n",
    "\n",
    "Из различий в синтаксисе, например, стоит подметить:\n",
    "\n",
    "* Для указания нужной оси координат в PyTorch используется `dim`, в отличие от `axis` в numpy\n",
    " * `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "* Приведение типов производится по-другому:\n",
    " * `x.astype('int64') -> x.type(torch.LongTensor)`\n",
    " \n",
    "И, конечно, numpy не позволяет использовать GPU и автоматически вычислять градиенты.\n",
    "\n",
    "Воспользуемся PyTorch на простом тригонометрическом примере (основанном на данной [публикации](https://www.quora.com/What-are-the-most-interesting-equation-plots)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим значения данной параметрической функции с помощью PyTorch:\n",
    "\n",
    "$$ x(t) = t - 1.5 * cos( 17 t) $$\n",
    "$$ y(t) = t - 1.5 * sin( 25 t) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-10, 10, steps = 10000)\n",
    "\n",
    "# compute x(t) and y(t) as defined above\n",
    "x = <your_code_here>\n",
    "y = <your_code_here>\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте также поменять параметры и пронаблюдать, как меняется график для их различных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод обратного распространения ошибки и автоматическое вычисление градиентов в PyTorch\n",
    "\n",
    "Как уже не раз упоминалось, PyTorch позволяет автоматически вычислять градиенты.  Каждый \"тензор\" в PyTorch обладает булевым свойством `requires_grad`. Если ему присвоено значение `True`, то по данному тензору может быть посчитан градиент.\n",
    "\n",
    "Общая процедура выглядит следующим образом:\n",
    "* Пусть `x = torch.arange(5, requires_grad=True)`.\n",
    "* Результат операции суммирования будет равен `b = torch.sum(x)`.\n",
    "* Для вычисления градиентов достаточно вызвать метод `b.backward()` у результата операции суммирования.\n",
    "* После этого у `x` появятся градиенты $\\frac{\\delta b}{\\delta x}$.\n",
    "\n",
    "Также стоит помнить, что если `x` одновременно участвует в нескольких операциях, то градиенты от них __суммируются__.\n",
    "\n",
    "\n",
    "Рассмотрим на интерактивном примере:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Упрощенная задача оценки стоимости жилья\n",
    "Воспользуемся рассмотренными методами для решения задачи оценки стоимости жилья с использованием набора данных _Boston house-prices dataset_. Для начала рассмотрим одномерный случай."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:,-1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "loss = torch.mean( (y_pred - y)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagete gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты можно увидеть в поле `.grad` у всех переменных, которые их \"требуют\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dL/dw = {}\\n\".format(w.grad))\n",
    "print(\"dL/db = {}\\n\".format(b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ввиду накопления градиентов по умолчанию, __очищать их между шагами градиентного спуска необходимо вручную__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:,-1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    y_pred = w * torch.log(x) + b\n",
    "    loss = torch.mean( (y - y_pred)**2 ) \n",
    "    loss.backward()\n",
    "\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i+1)%5==0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.scatter(x.data.numpy(), y_pred.data.numpy(), color='orange', linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy())\n",
    "        if loss.data.numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сходится и даже делает предсказания неплохого качества. Но, судя по графику, зависимость явно не является линейной.\n",
    "\n",
    "__Задача__: Попробуйте доработать код выше, чтобы регрессионная модель стала нелинейной. Помните о том, что вычисление градиентов происходит автоматически, т.е. достаточно лишь поменять способ вычисления `y_pred`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ \n",
    "\n",
    "___ \n",
    "\n",
    "___\n",
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "После того, как вы справились с задачей выше, полезно обратить внимание на один небольшой, но очень важный момент: все значения тензора `x` были поделены на \"волшебную\" константу 10. Посмотрим, что произойдет, если этого не делать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do not divide the x values by 10. See what happens next\n",
    "x = torch.tensor(boston.data[:,-1], dtype=torch.float32)\n",
    "\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "\n",
    "grad_history = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    y_pred = w * x  + b\n",
    "    loss = torch.mean( (y_pred - y)**2 )\n",
    "    loss.backward()\n",
    "    grad_history.append((w.grad.item(), b.grad.item()))\n",
    "\n",
    "    # Be extremely careful with accessing the .data attribute!\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i+1)%5==0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.scatter(x.data.numpy(), y_pred.data.numpy(), color='orange', linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy())\n",
    "        if loss.data.numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.log(np.abs(element[0])) for element in grad_history], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grad_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы могли заметить, мы столкнулись с \"взрывом градиентов\" (exloding gradients). Данная проблема является достаточно частой, и для ее решения используется нормировка данных, понижение learning rate или же техника _gradient clipping_, которая будет рассмотрена позднее.\n",
    "\n",
    "Константа 10 в данном случае была подобрана вручную. Лучше пользоваться классическими методами нормировки, например вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normed = boston.data.copy()[:, -1]\n",
    "x_normed -= np.mean(x_normed)\n",
    "x_normed /= np.std(x_normed)\n",
    "print(x_normed.mean(axis=0, keepdims=True).shape)\n",
    "print(x_normed.std(axis=0, keepdims=True).shape)\n",
    "boston.data[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или же с использованием средств `sklearn`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(boston.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или с использованием средств PyTorch и др.\n",
    "\n",
    "Из примера выше можно сделать простой вывод:\n",
    "\n",
    "__Будьте осторожны при использовании градиентных методов оптимизации. Даже в случае использования простых моделей (например, линейной регрессии), требуется аккуратная настройка гиперпараметров и качественная предобработка данных.__\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, при аккуратном использовании PyTorch может вычислять производные для очень сложных выражений. Для более детального разбора данного механизма обратимся к слайдам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Go to slides__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Реальная задача и высокоуровневое API PyTorch\n",
    "Наконец, применим полученные знания для решения задачи предсказания цены квартиры уже с использованием всех данных. В этом нам поможет высокоуровневое API PyTorch, которым мы воспользуемся далее.\n",
    "\n",
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз обратимся к стуктуре базового класса `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим наши полные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)[:, None]\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим простейшую линейную модель. Для объединения нескольких последовательных преобразований можно воспользоваться `nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.Sequential.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model instance\n",
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('l1', nn.Linear(NUM_FEATURES, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На текущий момент в нашей модели лишь один слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим поведение нашей модели на данных подходящей размерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = X_train_torch[:3]\n",
    "example_target = y_train_torch[:3]\n",
    "\n",
    "# compute outputs given inputs, both are variables\n",
    "y_predicted = model(example_input)\n",
    "\n",
    "y_predicted # display what we've got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции потерь воспользуемся среднеквадратичной ошибкой MSE. Она также уже реализована в виде класса в PyTorch: `nn.MSELoss` (или в виде функции `F.mse_loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(y_predicted, example_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы сможем вычислить градиенты простым вызовом `loss.backward()`. Но перед этим введем последнее улучшение в базовый pipeline для одномерного случая. В нем мы реализовывали градиентный спуск вручную. Конечно, на практике используются уже реализованные механизмы оптимизации, такие как `SGD, Momentum, RMSProp, Adam` и другие. Они доступны в `torch.opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradients\n",
    "loss.backward()   \n",
    "\n",
    "# Make a step\n",
    "opt.step()          \n",
    "\n",
    "# Remove the gradients from the previous step\n",
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, нейронная сеть отлажена и мы можем запустить процедуру обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 150\n",
    "train_loss_history, test_loss_history = [], []\n",
    "\n",
    "for epoch_num in range(NUM_EPOCH):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    random_indices = np.random.choice(np.arange(len(X_train_torch)), size=32)\n",
    "    X_train_torch_batch = X_train_torch[random_indices]\n",
    "    y_train_torch_batch = y_train_torch[random_indices]\n",
    "    y_predicted = model(X_train_torch_batch)\n",
    "    loss = loss_function(y_predicted, y_train_torch_batch)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    train_loss_history.append(loss.item()) # Always use .item() to store scalars in logs!\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_predicted_test = model(X_test_torch)\n",
    "        test_loss_history.append(loss_function(y_predicted_test, y_test_torch).item())\n",
    "    # the rest of code is just bells and whistles\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.scatter(np.arange(len(train_loss_history)), train_loss_history, label='train')\n",
    "    plt.scatter(np.arange(len(test_loss_history)), test_loss_history, label='test')\n",
    "    plt.title('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ввиду небольших размеров датасета, мы оценивали значение функции потерь и градиенты сразу на всех выборке на каждом шаге. __При работе с крупными датасетами необходимо разбиение на подвыборки (батчи)__. Аналогичное разбиение мы использовали в предыдущем занятии и еще не раз с ним столкнемся.\n",
    "\n",
    "Теперь вернемся выше и попробуем теперь сделать нейронную сеть глубже используя несколько слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final NN MSE on train data: {:.3f}'.format(train_loss_history[-1]))\n",
    "print('Final NN MSE on test data: {:.3f}'.format(test_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним полученные результаты с `Random Forest` и линейной регрессией из `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sklearn = LinearRegression().fit(X_train, y_train)\n",
    "print('Final sklearn Linear Regression MSE on train data: {:.3f}'.format(\n",
    "    mean_squared_error(lr_sklearn.predict(X_train), y_train)\n",
    "))\n",
    "print('Final sklearn Linear Regression MSE on test data: {:.3f}'.format(\n",
    "    mean_squared_error(lr_sklearn.predict(X_test), y_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sklearn = RandomForestRegressor(n_estimators=50).fit(X_train, y_train)\n",
    "print('Final sklearn Random Forest MSE on train data: {:.3f}'.format(\n",
    "    mean_squared_error(rf_sklearn.predict(X_train), y_train)\n",
    "))\n",
    "print('Final sklearn Random Forest MSE on test data: {:.3f}'.format(\n",
    "    mean_squared_error(rf_sklearn.predict(X_test), y_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, результаты линейной модели в PyTorch сопоставимы с результатами `sklearn`. Но линейная модель – лишь частный, простейший случай нейронной сети, и PyTorch позволяет с легкостью дорабатывать ее, в то время как в `sklearn` для учета нелинейных зависимостей необходимо генерировать признаки вручную или же использовать другие классы моделей (например, ансамбли деревьев)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "* Механизм обратного распространения ошибки (backprop) – основной принцип обучения всех современный нейронных сетей. В его основе лежит простейшая математика.\n",
    "* Отличная статья на тему понимания backprop за авторством Andrej Karpathy (один из авторов курса Stanford CS231n, ex. Tesla Head of AI): [Yes, you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "* Функции активации позволяют нейронным сетям автоматически выделять нелинейные зависимости между признаками и использовать их для решения задачи.\n",
    "* Различные функции активации обладают различными свойствами. О них стоит помнить, используя их на практике. Отличное решение по умолчанию: ReLU.\n",
    "* Благодаря механизму backpropagation и использованию нелинейных функций активации, нейронные сети способны самостоятельно выучивать информативные промежуточные представления данных, что является одним из наиболее важных отличий от \"классических\" моделей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 Research",
   "language": "python",
   "name": "py3_research_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
